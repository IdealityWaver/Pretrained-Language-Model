{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d8e792d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (BertConfig,\n",
    "                                  BertForSequenceClassification, BertTokenizer,\n",
    "                                  RobertaConfig,\n",
    "                                  RobertaForSequenceClassification,\n",
    "                                  RobertaTokenizer,\n",
    "                          )\n",
    "\n",
    "from transformers import glue_compute_metrics as compute_metrics\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from transformers import glue_processors as processors\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "\n",
    "from sti_plan import plan\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4e5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47384d",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da354760",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/24/2023 20:24:54 - INFO - __main__ -   device: cpu, n_gpu: 0\n",
      "12/24/2023 20:24:54 - INFO - transformers.configuration_utils -   loading configuration file models\\RTE\\32_32\\config.json\n",
      "12/24/2023 20:24:54 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"rte\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_intermediate\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "12/24/2023 20:24:54 - INFO - transformers.tokenization_utils -   Model name 'models\\RTE\\32_32' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased). Assuming 'models\\RTE\\32_32' is a path or url to a directory containing tokenizer files.\n",
      "12/24/2023 20:24:54 - INFO - transformers.tokenization_utils -   Didn't find file models\\RTE\\32_32\\added_tokens.json. We won't load it.\n",
      "12/24/2023 20:24:54 - INFO - transformers.tokenization_utils -   Didn't find file models\\RTE\\32_32\\special_tokens_map.json. We won't load it.\n",
      "12/24/2023 20:24:54 - INFO - transformers.tokenization_utils -   Didn't find file models\\RTE\\32_32\\tokenizer_config.json. We won't load it.\n",
      "12/24/2023 20:24:54 - INFO - transformers.tokenization_utils -   loading file models\\RTE\\32_32\\vocab.txt\n",
      "12/24/2023 20:24:54 - INFO - transformers.tokenization_utils -   loading file None\n",
      "12/24/2023 20:24:54 - INFO - transformers.tokenization_utils -   loading file None\n",
      "12/24/2023 20:24:54 - INFO - transformers.tokenization_utils -   loading file None\n",
      "12/24/2023 20:24:54 - INFO - transformers.modeling_utils -   loading weights file models\\RTE\\32_32\\pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detecting outliers...\n",
      "masked:  72278\n",
      "o_group = 72278\n",
      "gobo qunatization. Bits =  3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.bool_. The only supported types are: double, float, float16, int64, int32, and uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-dbbcc8f9169f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[0menc_bits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0memb_bits\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquantize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_bits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\123\\Code\\Pretrained-Language-Model\\DynaBERT\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mquantize\u001b[1;34m(self, bits)\u001b[0m\n\u001b[0;32m    452\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer_norm_eps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_dropout_prob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mquantize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0m_quantize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgobo_quantize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetect_o\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\123\\Code\\Pretrained-Language-Model\\DynaBERT\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36m_quantize\u001b[1;34m(layer, quantize_f, detect_o, bits)\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[0mo_group\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_outliers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"o_group = %d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m         \u001b[0mo_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[0mnew_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquantize_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\123\\Code\\Pretrained-Language-Model\\DynaBERT\\transformers\\modeling_bert.py\u001b[0m in \u001b[0;36mgobo_quantize\u001b[1;34m(weights, o_idx, bits)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgobo_quantize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gobo qunatization. Bits = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[0mg_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0mo_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m     \u001b[0mg_group\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_group\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[0mbins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.bool_. The only supported types are: double, float, float16, int64, int32, and uint8."
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--data_dir\", default='GLUE\\RTE', type=str, required=False,\n",
    "                    help=\"The input data dir. Should contain the .tsv files (or other data files) for the task.\")\n",
    "parser.add_argument(\"--model_type\", default='bert', type=str, required=False,\n",
    "                    help=\"Model type selected in the list: \" + \", \".join(MODEL_CLASSES.keys()))\n",
    "parser.add_argument(\"--task_name\", default='RTE', type=str, required=False,\n",
    "                    help=\"The name of the task to train selected in the list: \" + \", \".join(processors.keys()))\n",
    "parser.add_argument(\"--output_dir\", default='output', type=str, required=False,\n",
    "                    help=\"The output directory where the model predictions will be written.\")\n",
    "parser.add_argument(\"--max_seq_length\", default=128, type=int,\n",
    "                    help=\"The maximum total input sequence length after tokenization. Sequences longer \"\n",
    "                         \"than this will be truncated, sequences shorter will be padded.\")\n",
    "parser.add_argument(\"--do_lower_case\", default=True,\n",
    "                    help=\"Set this flag if you are using an uncased model.\")\n",
    "parser.add_argument(\"--per_gpu_eval_batch_size\", default=128, type=int,\n",
    "                    help=\"Batch size per GPU/CPU for evaluation.\")\n",
    "parser.add_argument(\"--no_cuda\", action='store_true',\n",
    "                    help=\"Avoid using CUDA when available\")\n",
    "parser.add_argument('--seed', type=int, default=42,\n",
    "                    help=\"random seed for initialization\")\n",
    "parser.add_argument(\"--model_dir\", type=str, default='models',\n",
    "                    help=\"The teacher model dir.\")\n",
    "parser.add_argument('--depth_mult', type=str, default='1.',\n",
    "                    help=\"the possible depths used for training, e.g., '1.' is for default\")\n",
    "parser.add_argument('--width_mult', type=str, default='1.',\n",
    "                    help=\"the possible depths used for training, e.g., '1.' is for default\")\n",
    "parser.add_argument('--emb', type=int, default=32,\n",
    "                    help=\"Embedding quantization bits\")\n",
    "parser.add_argument('--enc', type=int, default=32,\n",
    "                    help=\"Encoder quantization bits\")\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "\n",
    "\n",
    "# liux: 准备对应任务的模型目录和device。\n",
    "args.model_dir = os.path.join(args.model_dir, args.task_name)\n",
    "model_root = args.model_dir\n",
    "bits_conf = str(args.emb) + '_' + str(args.enc)\n",
    "args.model_dir = os.path.join(model_root, bits_conf)\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device\n",
    "torch.cuda.empty_cache()\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.INFO)\n",
    "logger.info(\"device: %s, n_gpu: %s\", device, args.n_gpu, )\n",
    "set_seed(args)\n",
    "\n",
    "# liux: 准备GLUE任务数据集。\n",
    "args.task_name = args.task_name.lower()\n",
    "if args.task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (args.task_name))\n",
    "processor = processors[args.task_name]()\n",
    "args.output_mode = output_modes[args.task_name]\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)\n",
    "\n",
    "# liux: 读取对应量化位目录下的模型config、模型和tokenizer，并加载archive文件中的模型参数到模型实例中。\n",
    "args.model_type = args.model_type.lower()\n",
    "config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n",
    "config = config_class.from_pretrained(args.model_dir, num_labels=num_labels, finetuning_task=args.task_name)\n",
    "tokenizer = tokenizer_class.from_pretrained(args.model_dir, do_lower_case=args.do_lower_case)\n",
    "model = model_class.from_pretrained(args.model_dir, config=config)\n",
    "model.to(args.device)\n",
    "# liux: 更新模型encoder的量化位。\n",
    "model.bert.encoder.update_bit(args.enc)\n",
    "\n",
    "emb_bits = 3\n",
    "enc_bits = 2\n",
    "if emb_bits != 0:\n",
    "    model.bert.embeddings.quantize(emb_bits)\n",
    "\n",
    "model.bert.embeddings\n",
    "# if enc_bits != 0:\n",
    "#     model.bert.encoder.quantize(enc_bits)\n",
    "# model_save_dir = os.path.join(model_root, str(emb_bits) + '_' + str(enc_bits))\n",
    "# if not os.path.exists(model_save_dir):\n",
    "#         os.makedirs(model_save_dir)\n",
    "# model.save_pretrained(model_save_dir)\n",
    "# tokenizer.save_vocabulary(model_save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325febc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e087fbb",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
